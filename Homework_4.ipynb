{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ry2fjSW3OrLDyP4-I5WWFFuQQHLrfYGW",
      "authorship_tag": "ABX9TyP5kUyA8mKe0lgRt2U3M0ok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WyattRoss/csci4170/blob/main/Homework_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "zfEAXq-5t0jN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format of a recording is {digit}_{speaker}_{index}.wav\n",
        "speakers = [\"jackson\", \"nicolas\", \"theo\", \"yweweler\", \"george\", \"lucas\"]\n",
        "recordings_per_digit = 50 # 0-indexed\n",
        "\n",
        "def load_recording(path):\n",
        "  wave, sr = librosa.load(path, sr=None, mono=True)\n",
        "  wave = wave[::3] # shortening to make the data more manageable\n",
        "  mfcc_wave = librosa.feature.mfcc(y=wave, sr=8000, n_mfcc=13, n_fft=300)\n",
        "  mfcc_wave = np.pad(mfcc_wave, ((0, 0), (0, 20-mfcc_wave.shape[1])), mode=\"constant\")\n",
        "  return mfcc_wave\n",
        "\n",
        "def generate_dataset(basepath):\n",
        "  df = pd.DataFrame(columns=[\"digit\", \"speaker\", \"recording_index\", \"waveform\"])\n",
        "  for speaker in speakers:\n",
        "    for digit in range(10):\n",
        "      for i in range(50):\n",
        "        path = basepath + f\"{digit}_{speaker}_{i}.wav\"\n",
        "        recording = load_recording(path)\n",
        "        df.loc[len(df.index)] = [digit, speaker, i, recording]\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "WN39I4ORupAs"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recordings = generate_dataset(\"./drive/MyDrive/recordings/\")"
      ],
      "metadata": {
        "id": "x7SA65j5z154"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioRNN(nn.Module):\n",
        "  # Types: None, \"LSTM\", \"GRU\"\n",
        "  def __init__(self, input_size=13, hidden_size=128, num_layers=2, num_classes=10, rnn_type=\"DEFAULT\"):\n",
        "    super(AudioRNN, self).__init__()\n",
        "\n",
        "    if rnn_type.upper() == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
        "    elif rnn_type.upper() == \"GRU\":\n",
        "      self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
        "    else:\n",
        "      self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, _ = self.rnn(x)\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    return out\n",
        "  def train_model(self, train_loader, num_epochs=10, learning_rate=0.001, device='cpu', verbose=True, checkpoints=10):\n",
        "      self.to(device)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "      self.train()\n",
        "      for epoch in range(num_epochs):\n",
        "          total_loss = 0\n",
        "          for i, (inputs, labels) in enumerate(train_loader):\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              outputs = self(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              total_loss += loss.item()\n",
        "\n",
        "          if verbose and epoch % (num_epochs // checkpoints) == 0:\n",
        "              avg_loss = total_loss / len(train_loader)\n",
        "              print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "  def test_model(self, data_loader, device='cpu', verbose=True):\n",
        "    self.eval()\n",
        "    self.to(device)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = self(inputs)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "O_q1KGz06sh6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchAudioDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        x = torch.tensor(row['waveform'].T, dtype=torch.float32)  # Shape: [time, features]\n",
        "        y = torch.tensor(row['digit'], dtype=torch.long)\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "hyQFxjffhIY0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TorchAudioDataset(recordings)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "CwvwjKCHhP-Y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = train_test_split(recordings, test_size=0.2, stratify=recordings['digit'], random_state=42)\n",
        "\n",
        "train_loader = DataLoader(TorchAudioDataset(df_train), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(TorchAudioDataset(df_val), batch_size=32)"
      ],
      "metadata": {
        "id": "FLXA_KcIhR4v"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AudioRNN()\n",
        "model.train_model(train_loader, num_epochs=100, learning_rate=0.001, device='cpu', verbose=True)\n",
        "model.test_model(val_loader, device='cpu', verbose=True)"
      ],
      "metadata": {
        "id": "gSEKlGwlhj_c",
        "outputId": "fa5081b5-04dc-498c-db28-20691a5d1ec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.3087\n",
            "Epoch [11/100], Loss: 1.7860\n",
            "Epoch [21/100], Loss: 1.4840\n",
            "Epoch [31/100], Loss: 1.3052\n",
            "Epoch [41/100], Loss: 1.1906\n",
            "Epoch [51/100], Loss: 0.9962\n",
            "Epoch [61/100], Loss: 0.9614\n",
            "Epoch [71/100], Loss: 0.8829\n",
            "Epoch [81/100], Loss: 0.7948\n",
            "Epoch [91/100], Loss: 0.8506\n",
            "Accuracy: 59.67%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59.666666666666664"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelLSTM = AudioRNN(rnn_type=\"LSTM\")\n",
        "modelLSTM.train_model(train_loader, num_epochs=10, learning_rate=0.001, device='cpu', verbose=True)\n",
        "modelLSTM.test_model(val_loader, device='cpu', verbose=True)"
      ],
      "metadata": {
        "id": "qwLPmCDOjU1z",
        "outputId": "3a565154-651f-43c3-d85c-3ea46355224a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.3042\n",
            "Epoch [2/10], Loss: 2.1321\n",
            "Epoch [3/10], Loss: 1.8841\n",
            "Epoch [4/10], Loss: 1.7743\n",
            "Epoch [5/10], Loss: 1.6714\n",
            "Epoch [6/10], Loss: 1.4921\n",
            "Epoch [7/10], Loss: 1.4184\n",
            "Epoch [8/10], Loss: 1.3371\n",
            "Epoch [9/10], Loss: 1.2606\n",
            "Epoch [10/10], Loss: 1.0969\n",
            "Accuracy: 61.33%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61.333333333333336"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelGRU = AudioRNN(rnn_type=\"GRU\")\n",
        "modelGRU.train_model(train_loader, num_epochs=10, learning_rate=0.001, device='cpu', verbose=True)\n",
        "modelGRU.test_model(val_loader, device='cpu', verbose=True)"
      ],
      "metadata": {
        "id": "-vNeNo6qja1c",
        "outputId": "773d75a7-a280-4fb9-bb9b-418a73a855f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.2746\n",
            "Epoch [2/10], Loss: 1.8537\n",
            "Epoch [3/10], Loss: 1.3010\n",
            "Epoch [4/10], Loss: 1.0416\n",
            "Epoch [5/10], Loss: 0.8505\n",
            "Epoch [6/10], Loss: 0.7091\n",
            "Epoch [7/10], Loss: 0.6081\n",
            "Epoch [8/10], Loss: 0.5073\n",
            "Epoch [9/10], Loss: 0.4408\n",
            "Epoch [10/10], Loss: 0.3992\n",
            "Accuracy: 82.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82.0"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vanilla RNNs suffer from the vanishing/exploding gradient problem, which makes it hard to learn long-term dependencies in sequences.\n",
        "- They rely solely on a single hidden state updated at each time step, without any mechanism to control what information is remembered or forgotten.\n",
        "- During backpropagation, gradients shrink or grow exponentially over time steps, leading to unstable or very slow training.\n",
        "- LSTMs introduce gates (input, forget, output) and a dedicated cell state that allow the network to learn what information to keep or discard across time.\n",
        "- GRUs simplify LSTMs by using fewer gates (update and reset), combining hidden and cell states, and typically converge faster with similar or better performance on smaller datasets.\n",
        "- Both LSTMs and GRUs preserve gradient flow better, leading to more stable and faster training than vanilla RNNs.\n",
        "\n",
        "A traditional feed-forward network could probably work for this problem. Performance will probably be worse though. We'd have to make sure all of the inputs are the same size, and then flatten them fully. Still, we'd lack actual temporal encoding, so we'd lose key context that the RNN variants thrive on.\n",
        "\n",
        "\n",
        "The dataset in use is the [free spoken digit dataset](https://github.com/Jakobovski/free-spoken-digit-dataset/tree/master)"
      ],
      "metadata": {
        "id": "dkC48mXHlmgn"
      }
    }
  ]
}